{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T13:38:10.738129Z",
     "start_time": "2018-04-29T13:38:10.533309Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "\n",
    "import blaze as bz\n",
    "import itertools\n",
    "from nose.tools import assert_true\n",
    "from parameterized import parameterized\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_equal, assert_almost_equal\n",
    "import pandas as pd\n",
    "from toolz import merge\n",
    "\n",
    "from zipline.pipeline import SimplePipelineEngine, Pipeline, CustomFactor\n",
    "from zipline.pipeline.common import (\n",
    "    EVENT_DATE_FIELD_NAME,\n",
    "    FISCAL_QUARTER_FIELD_NAME,\n",
    "    FISCAL_YEAR_FIELD_NAME,\n",
    "    SID_FIELD_NAME,\n",
    "    TS_FIELD_NAME,\n",
    ")\n",
    "from zipline.pipeline.data import DataSet\n",
    "from zipline.pipeline.data import Column\n",
    "from zipline.pipeline.loaders.blaze.estimates import (\n",
    "    BlazeNextEstimatesLoader,\n",
    "    BlazeNextSplitAdjustedEstimatesLoader,\n",
    "    BlazePreviousEstimatesLoader,\n",
    "    BlazePreviousSplitAdjustedEstimatesLoader,\n",
    ")\n",
    "from zipline.pipeline.loaders.earnings_estimates import (\n",
    "    INVALID_NUM_QTRS_MESSAGE,\n",
    "    NextEarningsEstimatesLoader,\n",
    "    NextSplitAdjustedEarningsEstimatesLoader,\n",
    "    normalize_quarters,\n",
    "    PreviousEarningsEstimatesLoader,\n",
    "    PreviousSplitAdjustedEarningsEstimatesLoader,\n",
    "    split_normalized_quarters,\n",
    ")\n",
    "from zipline.testing.fixtures import (\n",
    "    WithAdjustmentReader,\n",
    "    WithTradingSessions,\n",
    "    ZiplineTestCase,\n",
    ")\n",
    "from zipline.testing.predicates import assert_equal, assert_raises_regex\n",
    "from zipline.testing.predicates import assert_frame_equal\n",
    "from zipline.utils.numpy_utils import datetime64ns_dtype\n",
    "from zipline.utils.numpy_utils import float64_dtype\n",
    "\n",
    "\n",
    "class Estimates(DataSet):\n",
    "    event_date = Column(dtype=datetime64ns_dtype)\n",
    "    fiscal_quarter = Column(dtype=float64_dtype)\n",
    "    fiscal_year = Column(dtype=float64_dtype)\n",
    "    estimate = Column(dtype=float64_dtype)\n",
    "\n",
    "\n",
    "class MultipleColumnsEstimates(DataSet):\n",
    "    event_date = Column(dtype=datetime64ns_dtype)\n",
    "    fiscal_quarter = Column(dtype=float64_dtype)\n",
    "    fiscal_year = Column(dtype=float64_dtype)\n",
    "    estimate1 = Column(dtype=float64_dtype)\n",
    "    estimate2 = Column(dtype=float64_dtype)\n",
    "\n",
    "\n",
    "def QuartersEstimates(announcements_out):\n",
    "    class QtrEstimates(Estimates):\n",
    "        num_announcements = announcements_out\n",
    "        name = Estimates\n",
    "    return QtrEstimates\n",
    "\n",
    "\n",
    "def MultipleColumnsQuartersEstimates(announcements_out):\n",
    "    class QtrEstimates(MultipleColumnsEstimates):\n",
    "        num_announcements = announcements_out\n",
    "        name = Estimates\n",
    "    return QtrEstimates\n",
    "\n",
    "\n",
    "def QuartersEstimatesNoNumQuartersAttr(num_qtr):\n",
    "    class QtrEstimates(Estimates):\n",
    "        name = Estimates\n",
    "    return QtrEstimates\n",
    "\n",
    "\n",
    "def create_expected_df_for_factor_compute(start_date,\n",
    "                                          sids,\n",
    "                                          tuples,\n",
    "                                          end_date):\n",
    "    \"\"\"\n",
    "    Given a list of tuples of new data we get for each sid on each critical\n",
    "    date (when information changes), create a DataFrame that fills that\n",
    "    data through a date range ending at `end_date`.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(tuples,\n",
    "                      columns=[SID_FIELD_NAME,\n",
    "                               'estimate',\n",
    "                               'knowledge_date'])\n",
    "    df = df.pivot_table(columns=SID_FIELD_NAME,\n",
    "                        values='estimate',\n",
    "                        index='knowledge_date')\n",
    "    df = df.reindex(\n",
    "        pd.date_range(start_date, end_date)\n",
    "    )\n",
    "    # Index name is lost during reindex.\n",
    "    df.index = df.index.rename('knowledge_date')\n",
    "    df['at_date'] = end_date.tz_localize('utc')\n",
    "    df = df.set_index(['at_date', df.index.tz_localize('utc')]).ffill()\n",
    "    new_sids = set(sids) - set(df.columns)\n",
    "    df = df.reindex(columns=df.columns.union(new_sids))\n",
    "    return df\n",
    "\n",
    "\n",
    "class WithEstimates(WithTradingSessions, WithAdjustmentReader):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing cls.loader and cls.events as class\n",
    "    level fixtures.\n",
    "\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_loader(events, columns) -> PipelineLoader\n",
    "        Method which returns the loader to be used throughout tests.\n",
    "\n",
    "        events : pd.DataFrame\n",
    "            The raw events to be used as input to the pipeline loader.\n",
    "        columns : dict[str -> str]\n",
    "            The dictionary mapping the names of BoundColumns to the\n",
    "            associated column name in the events DataFrame.\n",
    "    make_columns() -> dict[BoundColumn -> str]\n",
    "       Method which returns a dictionary of BoundColumns mapped to the\n",
    "       associated column names in the raw data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Short window defined in order for test to run faster.\n",
    "    START_DATE = pd.Timestamp('2014-12-28')\n",
    "    END_DATE = pd.Timestamp('2015-02-04')\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        raise NotImplementedError('make_loader')\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        raise NotImplementedError('make_events')\n",
    "\n",
    "    @classmethod\n",
    "    def get_sids(cls):\n",
    "        return cls.events[SID_FIELD_NAME].unique()\n",
    "\n",
    "    @classmethod\n",
    "    def make_columns(cls):\n",
    "        return {\n",
    "            Estimates.event_date: 'event_date',\n",
    "            Estimates.fiscal_quarter: 'fiscal_quarter',\n",
    "            Estimates.fiscal_year: 'fiscal_year',\n",
    "            Estimates.estimate: 'estimate'\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        cls.events = cls.make_events()\n",
    "        cls.ASSET_FINDER_EQUITY_SIDS = cls.get_sids()\n",
    "        cls.ASSET_FINDER_EQUITY_SYMBOLS = [\n",
    "            's' + str(n) for n in cls.ASSET_FINDER_EQUITY_SIDS\n",
    "        ]\n",
    "        # We need to instantiate certain constants needed by supers of\n",
    "        # `WithEstimates` before we call their `init_class_fixtures`.\n",
    "        super(WithEstimates, cls).init_class_fixtures()\n",
    "        cls.columns = cls.make_columns()\n",
    "        # Some tests require `WithAdjustmentReader` to be set up by the time we\n",
    "        # make the loader.\n",
    "        cls.loader = cls.make_loader(cls.events, {column.name: val for\n",
    "                                                  column, val in\n",
    "                                                  cls.columns.items()})\n",
    "\n",
    "\n",
    "class WithOneDayPipeline(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing cls.events as a class level fixture and\n",
    "    defining a test for all inheritors to use.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        A simple DataFrame with columns needed for estimates and a single sid\n",
    "        and no other data.\n",
    "\n",
    "    Tests\n",
    "    ------\n",
    "    test_wrong_num_announcements_passed()\n",
    "        Tests that loading with an incorrect quarter number raises an error.\n",
    "    test_no_num_announcements_attr()\n",
    "        Tests that the loader throws an AssertionError if the dataset being\n",
    "        loaded has no `num_announcements` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_columns(cls):\n",
    "        return {\n",
    "            MultipleColumnsEstimates.event_date: 'event_date',\n",
    "            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',\n",
    "            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',\n",
    "            MultipleColumnsEstimates.estimate1: 'estimate1',\n",
    "            MultipleColumnsEstimates.estimate2: 'estimate2'\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        return pd.DataFrame({\n",
    "            SID_FIELD_NAME: [0] * 2,\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),\n",
    "                            pd.Timestamp('2015-01-06')],\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),\n",
    "                                    pd.Timestamp('2015-01-20')],\n",
    "            'estimate1': [1., 2.],\n",
    "            'estimate2': [3., 4.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: [2015, 2015]\n",
    "        })\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        raise NotImplementedError('make_expected_out')\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(WithOneDayPipeline, cls).init_class_fixtures()\n",
    "        cls.sid0 = cls.asset_finder.retrieve_asset(0)\n",
    "        cls.expected_out = cls.make_expected_out()\n",
    "\n",
    "    def test_load_one_day(self):\n",
    "        # We want to test multiple columns\n",
    "        dataset = MultipleColumnsQuartersEstimates(1)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "\n",
    "        results = engine.run_pipeline(\n",
    "            Pipeline({c.name: c.latest for c in dataset.columns}),\n",
    "            start_date=pd.Timestamp('2015-01-15', tz='utc'),\n",
    "            end_date=pd.Timestamp('2015-01-15', tz='utc'),\n",
    "        )\n",
    "        assert_frame_equal(results, self.expected_out)\n",
    "\n",
    "\n",
    "class PreviousWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    Tests that previous quarter loader correctly breaks if an incorrect\n",
    "    number of quarters is passed.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-10'),\n",
    "                'estimate1': 1.,\n",
    "                'estimate2': 3.,\n",
    "                FISCAL_QUARTER_FIELD_NAME: 1.,\n",
    "                FISCAL_YEAR_FIELD_NAME: 2015.,\n",
    "            },\n",
    "            index=pd.MultiIndex.from_tuples(\n",
    "                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class NextWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    Tests that next quarter loader correctly breaks if an incorrect\n",
    "    number of quarters is passed.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-20'),\n",
    "                'estimate1': 2.,\n",
    "                'estimate2': 4.,\n",
    "                FISCAL_QUARTER_FIELD_NAME: 2.,\n",
    "                FISCAL_YEAR_FIELD_NAME: 2015.,\n",
    "            },\n",
    "            index=pd.MultiIndex.from_tuples(\n",
    "                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "dummy_df = pd.DataFrame({SID_FIELD_NAME: 0},\n",
    "                        columns=[SID_FIELD_NAME,\n",
    "                                 TS_FIELD_NAME,\n",
    "                                 EVENT_DATE_FIELD_NAME,\n",
    "                                 FISCAL_QUARTER_FIELD_NAME,\n",
    "                                 FISCAL_YEAR_FIELD_NAME,\n",
    "                                 'estimate'],\n",
    "                        index=[0])\n",
    "\n",
    "\n",
    "class WithWrongLoaderDefinition(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing cls.events as a class level fixture and\n",
    "    defining a test for all inheritors to use.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        A simple DataFrame with columns needed for estimates and a single sid\n",
    "        and no other data.\n",
    "\n",
    "    Tests\n",
    "    ------\n",
    "    test_wrong_num_announcements_passed()\n",
    "        Tests that loading with an incorrect quarter number raises an error.\n",
    "    test_no_num_announcements_attr()\n",
    "        Tests that the loader throws an AssertionError if the dataset being\n",
    "        loaded has no `num_announcements` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        return dummy_df\n",
    "\n",
    "    def test_wrong_num_announcements_passed(self):\n",
    "        bad_dataset1 = QuartersEstimates(-1)\n",
    "        bad_dataset2 = QuartersEstimates(-2)\n",
    "        good_dataset = QuartersEstimates(1)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        columns = {c.name + str(dataset.num_announcements): c.latest\n",
    "                   for dataset in (bad_dataset1,\n",
    "                                   bad_dataset2,\n",
    "                                   good_dataset)\n",
    "                   for c in dataset.columns}\n",
    "        p = Pipeline(columns)\n",
    "\n",
    "        with self.assertRaises(ValueError) as e:\n",
    "            engine.run_pipeline(\n",
    "                p,\n",
    "                start_date=self.trading_days[0],\n",
    "                end_date=self.trading_days[-1],\n",
    "            )\n",
    "            assert_raises_regex(e, INVALID_NUM_QTRS_MESSAGE % \"-1,-2\")\n",
    "\n",
    "    def test_no_num_announcements_attr(self):\n",
    "        dataset = QuartersEstimatesNoNumQuartersAttr(1)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        p = Pipeline({c.name: c.latest for c in dataset.columns})\n",
    "\n",
    "        with self.assertRaises(AttributeError):\n",
    "            engine.run_pipeline(\n",
    "                p,\n",
    "                start_date=self.trading_days[0],\n",
    "                end_date=self.trading_days[-1],\n",
    "            )\n",
    "\n",
    "\n",
    "class PreviousWithWrongNumQuarters(WithWrongLoaderDefinition,\n",
    "                                   ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    Tests that previous quarter loader correctly breaks if an incorrect\n",
    "    number of quarters is passed.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "\n",
    "class NextWithWrongNumQuarters(WithWrongLoaderDefinition,\n",
    "                               ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    Tests that next quarter loader correctly breaks if an incorrect\n",
    "    number of quarters is passed.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "\n",
    "options = [\"split_adjustments_loader\",\n",
    "           \"split_adjusted_column_names\",\n",
    "           \"split_adjusted_asof\"]\n",
    "\n",
    "\n",
    "class WrongSplitsLoaderDefinition(WithEstimates, ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    Test class that tests that loaders break correctly when incorrectly\n",
    "    instantiated.\n",
    "\n",
    "    Tests\n",
    "    -----\n",
    "    test_extra_splits_columns_passed(SplitAdjustedEstimatesLoader)\n",
    "        A test that checks that the loader correctly breaks when an\n",
    "        unexpected column is passed in the list of split-adjusted columns.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(WithEstimates, cls).init_class_fixtures()\n",
    "\n",
    "    @parameterized.expand(itertools.product(\n",
    "        (NextSplitAdjustedEarningsEstimatesLoader,\n",
    "         PreviousSplitAdjustedEarningsEstimatesLoader),\n",
    "    ))\n",
    "    def test_extra_splits_columns_passed(self, loader):\n",
    "        columns = {\n",
    "            Estimates.event_date: 'event_date',\n",
    "            Estimates.fiscal_quarter: 'fiscal_quarter',\n",
    "            Estimates.fiscal_year: 'fiscal_year',\n",
    "            Estimates.estimate: 'estimate'\n",
    "        }\n",
    "\n",
    "        with self.assertRaises(ValueError):\n",
    "            loader(dummy_df,\n",
    "                   {column.name: val for column, val in\n",
    "                    columns.items()},\n",
    "                   split_adjustments_loader=self.adjustment_reader,\n",
    "                   split_adjusted_column_names=[\"estimate\", \"extra_col\"],\n",
    "                   split_adjusted_asof=pd.Timestamp(\"2015-01-01\"))\n",
    "\n",
    "\n",
    "class WithEstimatesTimeZero(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing cls.events as a class level fixture and\n",
    "    defining a test for all inheritors to use.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cls.events : pd.DataFrame\n",
    "        Generated dynamically in order to test inter-leavings of estimates and\n",
    "        event dates for multiple quarters to make sure that we select the\n",
    "        right immediate 'next' or 'previous' quarter relative to each date -\n",
    "        i.e., the right 'time zero' on the timeline. We care about selecting\n",
    "        the right 'time zero' because we use that to calculate which quarter's\n",
    "        data needs to be returned for each day.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    get_expected_estimate(q1_knowledge,\n",
    "                          q2_knowledge,\n",
    "                          comparable_date) -> pd.DataFrame\n",
    "        Retrieves the expected estimate given the latest knowledge about each\n",
    "        quarter and the date on which the estimate is being requested. If\n",
    "        there is no expected estimate, returns an empty DataFrame.\n",
    "\n",
    "    Tests\n",
    "    ------\n",
    "    test_estimates()\n",
    "        Tests that we get the right 'time zero' value on each day for each\n",
    "        sid and for each column.\n",
    "    \"\"\"\n",
    "    # Shorter date range for performance\n",
    "    END_DATE = pd.Timestamp('2015-01-28')\n",
    "\n",
    "    q1_knowledge_dates = [pd.Timestamp('2015-01-01'),\n",
    "                          pd.Timestamp('2015-01-04'),\n",
    "                          pd.Timestamp('2015-01-07'),\n",
    "                          pd.Timestamp('2015-01-11')]\n",
    "    q2_knowledge_dates = [pd.Timestamp('2015-01-14'),\n",
    "                          pd.Timestamp('2015-01-17'),\n",
    "                          pd.Timestamp('2015-01-20'),\n",
    "                          pd.Timestamp('2015-01-23')]\n",
    "    # We want to model the possibility of an estimate predicting a release date\n",
    "    # that doesn't match the actual release. This could be done by dynamically\n",
    "    # generating more combinations with different release dates, but that\n",
    "    # significantly increases the amount of time it takes to run the tests.\n",
    "    # These hard-coded cases are sufficient to know that we can update our\n",
    "    # beliefs when we get new information.\n",
    "    q1_release_dates = [pd.Timestamp('2015-01-13'),\n",
    "                        pd.Timestamp('2015-01-14')]  # One day late\n",
    "    q2_release_dates = [pd.Timestamp('2015-01-25'),  # One day early\n",
    "                        pd.Timestamp('2015-01-26')]\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        \"\"\"\n",
    "        In order to determine which estimate we care about for a particular\n",
    "        sid, we need to look at all estimates that we have for that sid and\n",
    "        their associated event dates.\n",
    "\n",
    "        We define q1 < q2, and thus event1 < event2 since event1 occurs\n",
    "        during q1 and event2 occurs during q2 and we assume that there can\n",
    "        only be 1 event per quarter. We assume that there can be multiple\n",
    "        estimates per quarter leading up to the event. We assume that estimates\n",
    "        will not surpass the relevant event date. We will look at 2 estimates\n",
    "        for an event before the event occurs, since that is the simplest\n",
    "        scenario that covers the interesting edge cases:\n",
    "            - estimate values changing\n",
    "            - a release date changing\n",
    "            - estimates for different quarters interleaving\n",
    "\n",
    "        Thus, we generate all possible inter-leavings of 2 estimates per\n",
    "        quarter-event where estimate1 < estimate2 and all estimates are < the\n",
    "        relevant event and assign each of these inter-leavings to a\n",
    "        different sid.\n",
    "        \"\"\"\n",
    "\n",
    "        sid_estimates = []\n",
    "        sid_releases = []\n",
    "        # We want all permutations of 2 knowledge dates per quarter.\n",
    "        it = enumerate(\n",
    "            itertools.permutations(cls.q1_knowledge_dates +\n",
    "                                   cls.q2_knowledge_dates,\n",
    "                                   4)\n",
    "        )\n",
    "        for sid, (q1e1, q1e2, q2e1, q2e2) in it:\n",
    "            # We're assuming that estimates must come before the relevant\n",
    "            # release.\n",
    "            if (q1e1 < q1e2 and\n",
    "                    q2e1 < q2e2 and\n",
    "                    # All estimates are < Q2's event, so just constrain Q1\n",
    "                    # estimates.\n",
    "                    q1e1 < cls.q1_release_dates[0] and\n",
    "                    q1e2 < cls.q1_release_dates[0]):\n",
    "                sid_estimates.append(cls.create_estimates_df(q1e1,\n",
    "                                                             q1e2,\n",
    "                                                             q2e1,\n",
    "                                                             q2e2,\n",
    "                                                             sid))\n",
    "                sid_releases.append(cls.create_releases_df(sid))\n",
    "        return pd.concat(sid_estimates +\n",
    "                         sid_releases).reset_index(drop=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_sids(cls):\n",
    "        sids = cls.events[SID_FIELD_NAME].unique()\n",
    "        # Tack on an extra sid to make sure that sids with no data are\n",
    "        # included but have all-null columns.\n",
    "        return list(sids) + [max(sids) + 1]\n",
    "\n",
    "    @classmethod\n",
    "    def create_releases_df(cls, sid):\n",
    "        # Final release dates never change. The quarters have very tight date\n",
    "        # ranges in order to reduce the number of dates we need to iterate\n",
    "        # through when testing.\n",
    "        return pd.DataFrame({\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-13'),\n",
    "                            pd.Timestamp('2015-01-26')],\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-13'),\n",
    "                                    pd.Timestamp('2015-01-26')],\n",
    "            'estimate': [0.5, 0.8],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1.0, 2.0],\n",
    "            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0],\n",
    "            SID_FIELD_NAME: sid\n",
    "        })\n",
    "\n",
    "    @classmethod\n",
    "    def create_estimates_df(cls,\n",
    "                            q1e1,\n",
    "                            q1e2,\n",
    "                            q2e1,\n",
    "                            q2e2,\n",
    "                            sid):\n",
    "        return pd.DataFrame({\n",
    "            EVENT_DATE_FIELD_NAME: cls.q1_release_dates + cls.q2_release_dates,\n",
    "            'estimate': [.1, .2, .3, .4],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1.0, 1.0, 2.0, 2.0],\n",
    "            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0, 2015.0, 2015.0],\n",
    "            TS_FIELD_NAME: [q1e1, q1e2, q2e1, q2e2],\n",
    "            SID_FIELD_NAME: sid,\n",
    "        })\n",
    "\n",
    "    def get_expected_estimate(self,\n",
    "                              q1_knowledge,\n",
    "                              q2_knowledge,\n",
    "                              comparable_date):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def test_estimates(self):\n",
    "        dataset = QuartersEstimates(1)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        results = engine.run_pipeline(\n",
    "            Pipeline({c.name: c.latest for c in dataset.columns}),\n",
    "            start_date=self.trading_days[1],\n",
    "            end_date=self.trading_days[-2],\n",
    "        )\n",
    "        for sid in self.ASSET_FINDER_EQUITY_SIDS:\n",
    "            sid_estimates = results.xs(sid, level=1)\n",
    "            # Separate assertion for all-null DataFrame to avoid setting\n",
    "            # column dtypes on `all_expected`.\n",
    "            if sid == max(self.ASSET_FINDER_EQUITY_SIDS):\n",
    "                assert_true(sid_estimates.isnull().all().all())\n",
    "            else:\n",
    "                ts_sorted_estimates = self.events[\n",
    "                    self.events[SID_FIELD_NAME] == sid\n",
    "                ].sort(TS_FIELD_NAME)\n",
    "                q1_knowledge = ts_sorted_estimates[\n",
    "                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 1\n",
    "                ]\n",
    "                q2_knowledge = ts_sorted_estimates[\n",
    "                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 2\n",
    "                ]\n",
    "                all_expected = pd.concat(\n",
    "                    [self.get_expected_estimate(\n",
    "                        q1_knowledge[q1_knowledge[TS_FIELD_NAME] <=\n",
    "                                     date.tz_localize(None)],\n",
    "                        q2_knowledge[q2_knowledge[TS_FIELD_NAME] <=\n",
    "                                     date.tz_localize(None)],\n",
    "                        date.tz_localize(None),\n",
    "                    ).set_index([[date]]) for date in sid_estimates.index],\n",
    "                    axis=0)\n",
    "                assert_equal(all_expected[sid_estimates.columns],\n",
    "                             sid_estimates)\n",
    "\n",
    "\n",
    "class NextEstimate(WithEstimatesTimeZero, ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    def get_expected_estimate(self,\n",
    "                              q1_knowledge,\n",
    "                              q2_knowledge,\n",
    "                              comparable_date):\n",
    "        # If our latest knowledge of q1 is that the release is\n",
    "        # happening on this simulation date or later, then that's\n",
    "        # the estimate we want to use.\n",
    "        if (not q1_knowledge.empty and\n",
    "            q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] >=\n",
    "                comparable_date):\n",
    "            return q1_knowledge.iloc[-1:]\n",
    "        # If q1 has already happened or we don't know about it\n",
    "        # yet and our latest knowledge indicates that q2 hasn't\n",
    "        # happened yet, then that's the estimate we want to use.\n",
    "        elif (not q2_knowledge.empty and\n",
    "              q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] >=\n",
    "                comparable_date):\n",
    "            return q2_knowledge.iloc[-1:]\n",
    "        return pd.DataFrame(columns=q1_knowledge.columns,\n",
    "                            index=[comparable_date])\n",
    "\n",
    "\n",
    "class BlazeNextEstimateLoaderTestCase(NextEstimate):\n",
    "    \"\"\"\n",
    "    Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class PreviousEstimate(WithEstimatesTimeZero, ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    def get_expected_estimate(self,\n",
    "                              q1_knowledge,\n",
    "                              q2_knowledge,\n",
    "                              comparable_date):\n",
    "\n",
    "        # The expected estimate will be for q2 if the last thing\n",
    "        # we've seen is that the release date already happened.\n",
    "        # Otherwise, it'll be for q1, as long as the release date\n",
    "        # for q1 has already happened.\n",
    "        if (not q2_knowledge.empty and\n",
    "            q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] <=\n",
    "                comparable_date):\n",
    "            return q2_knowledge.iloc[-1:]\n",
    "        elif (not q1_knowledge.empty and\n",
    "              q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] <=\n",
    "                comparable_date):\n",
    "            return q1_knowledge.iloc[-1:]\n",
    "        return pd.DataFrame(columns=q1_knowledge.columns,\n",
    "                            index=[comparable_date])\n",
    "\n",
    "\n",
    "class BlazePreviousEstimateLoaderTestCase(PreviousEstimate):\n",
    "    \"\"\"\n",
    "    Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class WithEstimateMultipleQuarters(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing cls.events, cls.make_expected_out as\n",
    "    class-level fixtures and self.test_multiple_qtrs_requested as a test.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        Simple DataFrame with estimates for 2 quarters for a single sid.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_expected_out() --> pd.DataFrame\n",
    "        Returns the DataFrame that is expected as a result of running a\n",
    "        Pipeline where estimates are requested for multiple quarters out.\n",
    "    fill_expected_out(expected)\n",
    "        Fills the expected DataFrame with data.\n",
    "\n",
    "    Tests\n",
    "    ------\n",
    "    test_multiple_qtrs_requested()\n",
    "        Runs a Pipeline that calculate which estimates for multiple quarters\n",
    "        out and checks that the returned columns contain data for the correct\n",
    "        number of quarters out.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        return pd.DataFrame({\n",
    "            SID_FIELD_NAME: [0] * 2,\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),\n",
    "                            pd.Timestamp('2015-01-06')],\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),\n",
    "                                    pd.Timestamp('2015-01-20')],\n",
    "            'estimate': [1., 2.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: [2015, 2015]\n",
    "        })\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(WithEstimateMultipleQuarters, cls).init_class_fixtures()\n",
    "        cls.expected_out = cls.make_expected_out()\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        expected = pd.DataFrame(columns=[cls.columns[col] + '1'\n",
    "                                         for col in cls.columns] +\n",
    "                                        [cls.columns[col] + '2'\n",
    "                                         for col in cls.columns],\n",
    "                                index=cls.trading_days)\n",
    "\n",
    "        for (col, raw_name), suffix in itertools.product(\n",
    "            cls.columns.items(), ('1', '2')\n",
    "        ):\n",
    "            expected_name = raw_name + suffix\n",
    "            if col.dtype == datetime64ns_dtype:\n",
    "                expected[expected_name] = pd.to_datetime(\n",
    "                    expected[expected_name]\n",
    "                )\n",
    "            else:\n",
    "                expected[expected_name] = expected[\n",
    "                    expected_name\n",
    "                ].astype(col.dtype)\n",
    "        cls.fill_expected_out(expected)\n",
    "        return expected.reindex(cls.trading_days)\n",
    "\n",
    "    def test_multiple_qtrs_requested(self):\n",
    "        dataset1 = QuartersEstimates(1)\n",
    "        dataset2 = QuartersEstimates(2)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "\n",
    "        results = engine.run_pipeline(\n",
    "            Pipeline(\n",
    "                merge([{c.name + '1': c.latest for c in dataset1.columns},\n",
    "                       {c.name + '2': c.latest for c in dataset2.columns}])\n",
    "            ),\n",
    "            start_date=self.trading_days[0],\n",
    "            end_date=self.trading_days[-1],\n",
    "        )\n",
    "        q1_columns = [col.name + '1' for col in self.columns]\n",
    "        q2_columns = [col.name + '2' for col in self.columns]\n",
    "\n",
    "        # We now expect a column for 1 quarter out and a column for 2\n",
    "        # quarters out for each of the dataset columns.\n",
    "        assert_equal(sorted(np.array(q1_columns + q2_columns)),\n",
    "                     sorted(results.columns.values))\n",
    "        assert_equal(self.expected_out.sort(axis=1),\n",
    "                     results.xs(0, level=1).sort(axis=1))\n",
    "\n",
    "\n",
    "class NextEstimateMultipleQuarters(\n",
    "    WithEstimateMultipleQuarters, ZiplineTestCase\n",
    "):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def fill_expected_out(cls, expected):\n",
    "        # Fill columns for 1 Q out\n",
    "        for raw_name in cls.columns.values():\n",
    "            expected.loc[\n",
    "                pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-11'),\n",
    "                raw_name + '1'\n",
    "            ] = cls.events[raw_name].iloc[0]\n",
    "            expected.loc[\n",
    "                pd.Timestamp('2015-01-11'):pd.Timestamp('2015-01-20'),\n",
    "                raw_name + '1'\n",
    "            ] = cls.events[raw_name].iloc[1]\n",
    "\n",
    "        # Fill columns for 2 Q out\n",
    "        # We only have an estimate and event date for 2 quarters out before\n",
    "        # Q1's event happens; after Q1's event, we know 1 Q out but not 2 Qs\n",
    "        # out.\n",
    "        for col_name in ['estimate', 'event_date']:\n",
    "            expected.loc[\n",
    "                pd.Timestamp('2015-01-06'):pd.Timestamp('2015-01-10'),\n",
    "                col_name + '2'\n",
    "            ] = cls.events[col_name].iloc[1]\n",
    "        # But we know what FQ and FY we'd need in both Q1 and Q2\n",
    "        # because we know which FQ is next and can calculate from there\n",
    "        expected.loc[\n",
    "            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-09'),\n",
    "            FISCAL_QUARTER_FIELD_NAME + '2'\n",
    "        ] = 2\n",
    "        expected.loc[\n",
    "            pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20'),\n",
    "            FISCAL_QUARTER_FIELD_NAME + '2'\n",
    "        ] = 3\n",
    "        expected.loc[\n",
    "            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-20'),\n",
    "            FISCAL_YEAR_FIELD_NAME + '2'\n",
    "        ] = 2015\n",
    "\n",
    "        return expected\n",
    "\n",
    "\n",
    "class BlazeNextEstimateMultipleQuarters(NextEstimateMultipleQuarters):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class PreviousEstimateMultipleQuarters(\n",
    "    WithEstimateMultipleQuarters,\n",
    "    ZiplineTestCase\n",
    "):\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def fill_expected_out(cls, expected):\n",
    "        # Fill columns for 1 Q out\n",
    "        for raw_name in cls.columns.values():\n",
    "            expected[raw_name + '1'].loc[\n",
    "                pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-19')\n",
    "            ] = cls.events[raw_name].iloc[0]\n",
    "            expected[raw_name + '1'].loc[\n",
    "                pd.Timestamp('2015-01-20'):\n",
    "            ] = cls.events[raw_name].iloc[1]\n",
    "\n",
    "        # Fill columns for 2 Q out\n",
    "        for col_name in ['estimate', 'event_date']:\n",
    "            expected[col_name + '2'].loc[\n",
    "                pd.Timestamp('2015-01-20'):\n",
    "            ] = cls.events[col_name].iloc[0]\n",
    "        expected[\n",
    "            FISCAL_QUARTER_FIELD_NAME + '2'\n",
    "        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 4\n",
    "        expected[\n",
    "            FISCAL_YEAR_FIELD_NAME + '2'\n",
    "        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 2014\n",
    "        expected[\n",
    "            FISCAL_QUARTER_FIELD_NAME + '2'\n",
    "        ].loc[pd.Timestamp('2015-01-20'):] = 1\n",
    "        expected[\n",
    "            FISCAL_YEAR_FIELD_NAME + '2'\n",
    "        ].loc[pd.Timestamp('2015-01-20'):] = 2015\n",
    "        return expected\n",
    "\n",
    "\n",
    "class BlazePreviousEstimateMultipleQuarters(PreviousEstimateMultipleQuarters):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class WithVaryingNumEstimates(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing fixtures and a test to ensure that we\n",
    "    have the correct overwrites when the event date changes. We want to make\n",
    "    sure that if we have a quarter with an event date that gets pushed back,\n",
    "    we don't start overwriting for the next quarter early. Likewise,\n",
    "    if we have a quarter with an event date that gets pushed forward, we want\n",
    "    to make sure that we start applying adjustments at the appropriate, earlier\n",
    "    date, rather than the later date.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    assert_compute()\n",
    "        Defines how to determine that results computed for the `SomeFactor`\n",
    "        factor are correct.\n",
    "\n",
    "    Tests\n",
    "    -----\n",
    "    test_windows_with_varying_num_estimates()\n",
    "        Tests that we create the correct overwrites from 2015-01-13 to\n",
    "        2015-01-14 regardless of how event dates were updated for each\n",
    "        quarter for each sid.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        return pd.DataFrame({\n",
    "            SID_FIELD_NAME: [0] * 3 + [1] * 3,\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                            pd.Timestamp('2015-01-12'),\n",
    "                            pd.Timestamp('2015-01-13')] * 2,\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-12'),\n",
    "                                    pd.Timestamp('2015-01-13'),\n",
    "                                    pd.Timestamp('2015-01-20'),\n",
    "                                    pd.Timestamp('2015-01-13'),\n",
    "                                    pd.Timestamp('2015-01-12'),\n",
    "                                    pd.Timestamp('2015-01-20')],\n",
    "            'estimate': [11., 12., 21.] * 2,\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 1, 2] * 2,\n",
    "            FISCAL_YEAR_FIELD_NAME: [2015] * 6\n",
    "        })\n",
    "\n",
    "    @classmethod\n",
    "    def assert_compute(cls, estimate, today):\n",
    "        raise NotImplementedError('assert_compute')\n",
    "\n",
    "    def test_windows_with_varying_num_estimates(self):\n",
    "        dataset = QuartersEstimates(1)\n",
    "        assert_compute = self.assert_compute\n",
    "\n",
    "        class SomeFactor(CustomFactor):\n",
    "            inputs = [dataset.estimate]\n",
    "            window_length = 3\n",
    "\n",
    "            def compute(self, today, assets, out, estimate):\n",
    "                assert_compute(estimate, today)\n",
    "\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        engine.run_pipeline(\n",
    "            Pipeline({'est': SomeFactor()}),\n",
    "            start_date=pd.Timestamp('2015-01-13', tz='utc'),\n",
    "            # last event date we have\n",
    "            end_date=pd.Timestamp('2015-01-14', tz='utc'),\n",
    "        )\n",
    "\n",
    "\n",
    "class PreviousVaryingNumEstimates(\n",
    "    WithVaryingNumEstimates,\n",
    "    ZiplineTestCase\n",
    "):\n",
    "    def assert_compute(self, estimate, today):\n",
    "        if today == pd.Timestamp('2015-01-13', tz='utc'):\n",
    "            assert_array_equal(estimate[:, 0],\n",
    "                               np.array([np.NaN, np.NaN, 12]))\n",
    "            assert_array_equal(estimate[:, 1],\n",
    "                               np.array([np.NaN, 12, 12]))\n",
    "        else:\n",
    "            assert_array_equal(estimate[:, 0],\n",
    "                               np.array([np.NaN, 12, 12]))\n",
    "            assert_array_equal(estimate[:, 1],\n",
    "                               np.array([12, 12, 12]))\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "\n",
    "class BlazePreviousVaryingNumEstimates(PreviousVaryingNumEstimates):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class NextVaryingNumEstimates(\n",
    "    WithVaryingNumEstimates,\n",
    "    ZiplineTestCase\n",
    "):\n",
    "\n",
    "    def assert_compute(self, estimate, today):\n",
    "        if today == pd.Timestamp('2015-01-13', tz='utc'):\n",
    "            assert_array_equal(estimate[:, 0],\n",
    "                               np.array([11, 12, 12]))\n",
    "            assert_array_equal(estimate[:, 1],\n",
    "                               np.array([np.NaN, np.NaN, 21]))\n",
    "        else:\n",
    "            assert_array_equal(estimate[:, 0],\n",
    "                               np.array([np.NaN, 21, 21]))\n",
    "            assert_array_equal(estimate[:, 1],\n",
    "                               np.array([np.NaN, 21, 21]))\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "\n",
    "class BlazeNextVaryingNumEstimates(NextVaryingNumEstimates):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "        )\n",
    "\n",
    "\n",
    "class WithEstimateWindows(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing fixures and a test to test running a\n",
    "    Pipeline with an estimates loader over differently-sized windows.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    events : pd.DataFrame\n",
    "        DataFrame with estimates for 2 quarters for 2 sids.\n",
    "    window_test_start_date : pd.Timestamp\n",
    "        The date from which the window should start.\n",
    "    timelines : dict[int -> pd.DataFrame]\n",
    "        A dictionary mapping to the number of quarters out to\n",
    "        snapshots of how the data should look on each date in the date range.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_expected_timelines() -> dict[int -> pd.DataFrame]\n",
    "        Creates a dictionary of expected data. See `timelines`, above.\n",
    "\n",
    "    Tests\n",
    "    -----\n",
    "    test_estimate_windows_at_quarter_boundaries()\n",
    "        Tests that we overwrite values with the correct quarter's estimate at\n",
    "        the correct dates when we have a factor that asks for a window of data.\n",
    "    \"\"\"\n",
    "    END_DATE = pd.Timestamp('2015-02-10')\n",
    "    window_test_start_date = pd.Timestamp('2015-01-05')\n",
    "    critical_dates = [pd.Timestamp('2015-01-09', tz='utc'),\n",
    "                      pd.Timestamp('2015-01-15', tz='utc'),\n",
    "                      pd.Timestamp('2015-01-20', tz='utc'),\n",
    "                      pd.Timestamp('2015-01-26', tz='utc'),\n",
    "                      pd.Timestamp('2015-02-05', tz='utc'),\n",
    "                      pd.Timestamp('2015-02-10', tz='utc')]\n",
    "    # Starting date, number of announcements out.\n",
    "    window_test_cases = list(itertools.product(critical_dates, (1, 2)))\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        # Typical case: 2 consecutive quarters.\n",
    "        sid_0_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [cls.window_test_start_date,\n",
    "                            pd.Timestamp('2015-01-20'),\n",
    "                            pd.Timestamp('2015-01-12'),\n",
    "                            pd.Timestamp('2015-02-10'),\n",
    "                            # We want a case where we get info for a later\n",
    "                            # quarter before the current quarter is over but\n",
    "                            # after the split_asof_date to make sure that\n",
    "                            # we choose the correct date to overwrite until.\n",
    "                            pd.Timestamp('2015-01-18')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-20'),\n",
    "                 pd.Timestamp('2015-01-20'),\n",
    "                 pd.Timestamp('2015-02-10'),\n",
    "                 pd.Timestamp('2015-02-10'),\n",
    "                 pd.Timestamp('2015-04-01')],\n",
    "            'estimate': [100., 101.] + [200., 201.] + [400],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2 + [4],\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 0,\n",
    "        })\n",
    "\n",
    "        # We want a case where we skip a quarter. We never find out about Q2.\n",
    "        sid_10_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                            pd.Timestamp('2015-01-12'),\n",
    "                            pd.Timestamp('2015-01-09'),\n",
    "                            pd.Timestamp('2015-01-15')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-22'), pd.Timestamp('2015-01-22'),\n",
    "                 pd.Timestamp('2015-02-05'), pd.Timestamp('2015-02-05')],\n",
    "            'estimate': [110., 111.] + [310., 311.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [3] * 2,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 10\n",
    "        })\n",
    "\n",
    "        # We want to make sure we have correct overwrites when sid quarter\n",
    "        # boundaries collide. This sid's quarter boundaries collide with sid 0.\n",
    "        sid_20_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [cls.window_test_start_date,\n",
    "                            pd.Timestamp('2015-01-07'),\n",
    "                            cls.window_test_start_date,\n",
    "                            pd.Timestamp('2015-01-17')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-20'),\n",
    "                 pd.Timestamp('2015-01-20'),\n",
    "                 pd.Timestamp('2015-02-10'),\n",
    "                 pd.Timestamp('2015-02-10')],\n",
    "            'estimate': [120., 121.] + [220., 221.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 20\n",
    "        })\n",
    "        concatted = pd.concat([sid_0_timeline,\n",
    "                               sid_10_timeline,\n",
    "                               sid_20_timeline]).reset_index()\n",
    "        np.random.seed(0)\n",
    "        return concatted.reindex(np.random.permutation(concatted.index))\n",
    "\n",
    "    @classmethod\n",
    "    def get_sids(cls):\n",
    "        sids = sorted(cls.events[SID_FIELD_NAME].unique())\n",
    "        # Add extra sids between sids in our data. We want to test that we\n",
    "        # apply adjustments to the correct sids.\n",
    "        return [sid for i in range(len(sids) - 1)\n",
    "                for sid in range(sids[i], sids[i+1])] + [sids[-1]]\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines(cls):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(WithEstimateWindows, cls).init_class_fixtures()\n",
    "        cls.create_expected_df_for_factor_compute = partial(\n",
    "            create_expected_df_for_factor_compute,\n",
    "            cls.window_test_start_date,\n",
    "            cls.get_sids()\n",
    "        )\n",
    "        cls.timelines = cls.make_expected_timelines()\n",
    "\n",
    "    @parameterized.expand(window_test_cases)\n",
    "    def test_estimate_windows_at_quarter_boundaries(self,\n",
    "                                                    start_date,\n",
    "                                                    num_announcements_out):\n",
    "        dataset = QuartersEstimates(num_announcements_out)\n",
    "        trading_days = self.trading_days\n",
    "        timelines = self.timelines\n",
    "        # The window length should be from the starting index back to the first\n",
    "        # date on which we got data. The goal is to ensure that as we\n",
    "        # progress through the timeline, all data we got, starting from that\n",
    "        # first date, is correctly overwritten.\n",
    "        window_len = (\n",
    "            self.trading_days.get_loc(start_date) -\n",
    "            self.trading_days.get_loc(self.window_test_start_date) + 1\n",
    "        )\n",
    "\n",
    "        class SomeFactor(CustomFactor):\n",
    "            inputs = [dataset.estimate]\n",
    "            window_length = window_len\n",
    "\n",
    "            def compute(self, today, assets, out, estimate):\n",
    "                today_idx = trading_days.get_loc(today)\n",
    "                today_timeline = timelines[\n",
    "                    num_announcements_out\n",
    "                ].loc[today].reindex(\n",
    "                    trading_days[:today_idx + 1]\n",
    "                ).values\n",
    "                timeline_start_idx = (len(today_timeline) - window_len)\n",
    "                assert_almost_equal(estimate,\n",
    "                                    today_timeline[timeline_start_idx:])\n",
    "\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        engine.run_pipeline(\n",
    "            Pipeline({'est': SomeFactor()}),\n",
    "            start_date=start_date,\n",
    "            # last event date we have\n",
    "            end_date=pd.Timestamp('2015-02-10', tz='utc'),\n",
    "        )\n",
    "\n",
    "\n",
    "class PreviousEstimateWindows(WithEstimateWindows, ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines(cls):\n",
    "        oneq_previous = pd.concat([\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute([\n",
    "                    (0, np.NaN, cls.window_test_start_date),\n",
    "                    (10, np.NaN, cls.window_test_start_date),\n",
    "                    (20, np.NaN, cls.window_test_start_date)\n",
    "                ], end_date)\n",
    "                for end_date in pd.date_range('2015-01-09', '2015-01-19')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-20'))],\n",
    "                pd.Timestamp('2015-01-20')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-20'))],\n",
    "                pd.Timestamp('2015-01-21')\n",
    "            ),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                     (10, 111, pd.Timestamp('2015-01-22')),\n",
    "                     (20, 121, pd.Timestamp('2015-01-20'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-22', '2015-02-04')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                     (10, 311, pd.Timestamp('2015-02-05')),\n",
    "                     (20, 121, pd.Timestamp('2015-01-20'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')\n",
    "                ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 201, pd.Timestamp('2015-02-10')),\n",
    "                 (10, 311, pd.Timestamp('2015-02-05')),\n",
    "                 (20, 221, pd.Timestamp('2015-02-10'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        twoq_previous = pd.concat(\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-09', '2015-02-09')] +\n",
    "            # We never get estimates for S1 for 2Q ago because once Q3\n",
    "            # becomes our previous quarter, 2Q ago would be Q2, and we have\n",
    "            # no data on it.\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 101, pd.Timestamp('2015-02-10')),\n",
    "                 (10, np.NaN, pd.Timestamp('2015-02-05')),\n",
    "                 (20, 121, pd.Timestamp('2015-02-10'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            )]\n",
    "        )\n",
    "        return {\n",
    "            1: oneq_previous,\n",
    "            2: twoq_previous\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazePreviousEstimateWindows(PreviousEstimateWindows):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousEstimatesLoader(bz.data(events), columns)\n",
    "\n",
    "\n",
    "class NextEstimateWindows(WithEstimateWindows, ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextEarningsEstimatesLoader(events, columns)\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines(cls):\n",
    "        oneq_next = pd.concat([\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100, cls.window_test_start_date),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (20, 120, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-07'))],\n",
    "                pd.Timestamp('2015-01-09')\n",
    "            ),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 100, cls.window_test_start_date),\n",
    "                     (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                     (20, 120, cls.window_test_start_date),\n",
    "                     (20, 121, pd.Timestamp('2015-01-07'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-12', '2015-01-19')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100, cls.window_test_start_date),\n",
    "                 (0, 101, pd.Timestamp('2015-01-20')),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 120, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-07'))],\n",
    "                pd.Timestamp('2015-01-20')\n",
    "            ),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                     (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                     (20, 220, cls.window_test_start_date),\n",
    "                     (20, 221, pd.Timestamp('2015-01-17'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-21', '2015-01-22')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                     (10, 310, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 311, pd.Timestamp('2015-01-15')),\n",
    "                     (20, 220, cls.window_test_start_date),\n",
    "                     (20, 221, pd.Timestamp('2015-01-17'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-23', '2015-02-05')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                     (10, np.NaN, cls.window_test_start_date),\n",
    "                     (20, 220, cls.window_test_start_date),\n",
    "                     (20, 221, pd.Timestamp('2015-01-17'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                 (0, 201, pd.Timestamp('2015-02-10')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220, cls.window_test_start_date),\n",
    "                 (20, 221, pd.Timestamp('2015-01-17'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        twoq_next = pd.concat(\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-09', '2015-01-11')] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-12', '2015-01-16')] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220, cls.window_test_start_date),\n",
    "                 (20, 221, pd.Timestamp('2015-01-17'))],\n",
    "                pd.Timestamp('2015-01-20')\n",
    "            )] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            1: oneq_next,\n",
    "            2: twoq_next\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazeNextEstimateWindows(NextEstimateWindows):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextEstimatesLoader(bz.data(events), columns)\n",
    "\n",
    "\n",
    "class WithSplitAdjustedWindows(WithEstimateWindows):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing fixures and a test to test running a\n",
    "    Pipeline with an estimates loader over differently-sized windows and with\n",
    "    split adjustments.\n",
    "    \"\"\"\n",
    "\n",
    "    split_adjusted_asof_date = pd.Timestamp('2015-01-14')\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        # Add an extra sid that has a release before the split-asof-date in\n",
    "        # order to test that we're reversing splits correctly in the previous\n",
    "        # case (without an overwrite) and in the next case (with an overwrite).\n",
    "        sid_30 = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [cls.window_test_start_date,\n",
    "                            pd.Timestamp('2015-01-09'),\n",
    "                            # For Q2, we want it to start early enough\n",
    "                            # that we can have several adjustments before\n",
    "                            # the end of the first quarter so that we\n",
    "                            # can test un-adjusting & readjusting with an\n",
    "                            # overwrite.\n",
    "                            cls.window_test_start_date,\n",
    "                            # We want the Q2 event date to be enough past\n",
    "                            # the split-asof-date that we can have\n",
    "                            # several splits and can make sure that they\n",
    "                            # are applied correctly.\n",
    "                            pd.Timestamp('2015-01-20')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-09'),\n",
    "                 pd.Timestamp('2015-01-09'),\n",
    "                 pd.Timestamp('2015-01-20'),\n",
    "                 pd.Timestamp('2015-01-20')],\n",
    "            'estimate': [130., 131., 230., 231.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 30\n",
    "        })\n",
    "\n",
    "        # An extra sid to test no splits before the split-adjusted-asof-date.\n",
    "        # We want an event before and after the split-adjusted-asof-date &\n",
    "        # timestamps for data points also before and after\n",
    "        # split-adjsuted-asof-date (but also before the split dates, so that\n",
    "        # we can test that splits actually get applied at the correct times).\n",
    "        sid_40 = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                            pd.Timestamp('2015-01-15')],\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                                    pd.Timestamp('2015-02-10')],\n",
    "            'estimate': [140., 240.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 40\n",
    "        })\n",
    "\n",
    "        # An extra sid to test all splits before the\n",
    "        # split-adjusted-asof-date. All timestamps should be before that date\n",
    "        # so that we have cases where we un-apply and re-apply splits.\n",
    "        sid_50 = pd.DataFrame({\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                            pd.Timestamp('2015-01-12')],\n",
    "            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),\n",
    "                                    pd.Timestamp('2015-02-10')],\n",
    "            'estimate': [150., 250.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 50\n",
    "        })\n",
    "\n",
    "        return pd.concat([\n",
    "            # Slightly hacky, but want to make sure we're using the same\n",
    "            # events as WithEstimateWindows.\n",
    "            cls.__base__.make_events(),\n",
    "            sid_30,\n",
    "            sid_40,\n",
    "            sid_50,\n",
    "        ])\n",
    "\n",
    "    @classmethod\n",
    "    def make_splits_data(cls):\n",
    "        # For sid 0, we want to apply a series of splits before and after the\n",
    "        #  split-adjusted-asof-date we well as between quarters (for the\n",
    "        # previous case, where we won't see any values until after the event\n",
    "        # happens).\n",
    "        sid_0_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 0,\n",
    "            'ratio': (-1., 2., 3., 4., 5., 6., 7., 100),\n",
    "            'effective_date': (pd.Timestamp('2014-01-01'),  # Filter out\n",
    "                               # Split before Q1 event & after first estimate\n",
    "                               pd.Timestamp('2015-01-07'),\n",
    "                               # Split before Q1 event\n",
    "                               pd.Timestamp('2015-01-09'),\n",
    "                               # Split before Q1 event\n",
    "                               pd.Timestamp('2015-01-13'),\n",
    "                               # Split before Q1 event\n",
    "                               pd.Timestamp('2015-01-15'),\n",
    "                               # Split before Q1 event\n",
    "                               pd.Timestamp('2015-01-18'),\n",
    "                               # Split after Q1 event and before Q2 event\n",
    "                               pd.Timestamp('2015-01-30'),\n",
    "                               # Filter out - this is after our date index\n",
    "                               pd.Timestamp('2016-01-01'))\n",
    "        })\n",
    "\n",
    "        sid_10_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 10,\n",
    "            'ratio': (.2, .3),\n",
    "            'effective_date': (\n",
    "                # We want a split before the first estimate and before the\n",
    "                # split-adjusted-asof-date but within our calendar index so\n",
    "                # that we can test that the split is NEVER applied.\n",
    "                pd.Timestamp('2015-01-07'),\n",
    "                # Apply a single split before Q1 event.\n",
    "                pd.Timestamp('2015-01-20')),\n",
    "        })\n",
    "\n",
    "        # We want a sid with split dates that collide with another sid (0) to\n",
    "        # make sure splits are correctly applied for both sids.\n",
    "        sid_20_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 20,\n",
    "            'ratio': (.4, .5, .6, .7, .8, .9,),\n",
    "            'effective_date': (\n",
    "                pd.Timestamp('2015-01-07'),\n",
    "                pd.Timestamp('2015-01-09'),\n",
    "                pd.Timestamp('2015-01-13'),\n",
    "                pd.Timestamp('2015-01-15'),\n",
    "                pd.Timestamp('2015-01-18'),\n",
    "                pd.Timestamp('2015-01-30')),\n",
    "        })\n",
    "\n",
    "        # This sid has event dates that are shifted back so that we can test\n",
    "        # cases where an event occurs before the split-asof-date.\n",
    "        sid_30_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 30,\n",
    "            'ratio': (8, 9, 10, 11, 12),\n",
    "            'effective_date': (\n",
    "                # Split before the event and before the\n",
    "                # split-asof-date.\n",
    "                pd.Timestamp('2015-01-07'),\n",
    "                # Split on date of event but before the\n",
    "                # split-asof-date.\n",
    "                pd.Timestamp('2015-01-09'),\n",
    "                # Split after the event, but before the\n",
    "                # split-asof-date.\n",
    "                pd.Timestamp('2015-01-13'),\n",
    "                pd.Timestamp('2015-01-15'),\n",
    "                pd.Timestamp('2015-01-18')),\n",
    "        })\n",
    "\n",
    "        # No splits for a sid before the split-adjusted-asof-date.\n",
    "        sid_40_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 40,\n",
    "            'ratio': (13, 14),\n",
    "            'effective_date': (\n",
    "                pd.Timestamp('2015-01-20'),\n",
    "                pd.Timestamp('2015-01-22')\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # No splits for a sid after the split-adjusted-asof-date.\n",
    "        sid_50_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 50,\n",
    "            'ratio': (15, 16),\n",
    "            'effective_date': (\n",
    "                pd.Timestamp('2015-01-13'),\n",
    "                pd.Timestamp('2015-01-14')\n",
    "            )\n",
    "        })\n",
    "\n",
    "        return pd.concat([\n",
    "            sid_0_splits,\n",
    "            sid_10_splits,\n",
    "            sid_20_splits,\n",
    "            sid_30_splits,\n",
    "            sid_40_splits,\n",
    "            sid_50_splits,\n",
    "        ])\n",
    "\n",
    "\n",
    "class PreviousWithSplitAdjustedWindows(WithSplitAdjustedWindows,\n",
    "                                       ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousSplitAdjustedEarningsEstimatesLoader(\n",
    "            events,\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof_date,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines(cls):\n",
    "        oneq_previous = pd.concat([\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute([\n",
    "                    (0, np.NaN, cls.window_test_start_date),\n",
    "                    (10, np.NaN, cls.window_test_start_date),\n",
    "                    (20, np.NaN, cls.window_test_start_date),\n",
    "                    # Undo all adjustments that haven't happened yet.\n",
    "                    (30, 131*1/10, pd.Timestamp('2015-01-09')),\n",
    "                    (40, 140., pd.Timestamp('2015-01-09')),\n",
    "                    (50, 150 * 1 / 15 * 1 / 16, pd.Timestamp('2015-01-09')),\n",
    "                ], end_date)\n",
    "                for end_date in pd.date_range('2015-01-09', '2015-01-12')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute([\n",
    "                (0, np.NaN, cls.window_test_start_date),\n",
    "                (10, np.NaN, cls.window_test_start_date),\n",
    "                (20, np.NaN, cls.window_test_start_date),\n",
    "                (30, 131, pd.Timestamp('2015-01-09')),\n",
    "                (40, 140., pd.Timestamp('2015-01-09')),\n",
    "                (50, 150. * 1 / 16, pd.Timestamp('2015-01-09')),\n",
    "            ], pd.Timestamp('2015-01-13')),\n",
    "            cls.create_expected_df_for_factor_compute([\n",
    "                (0, np.NaN, cls.window_test_start_date),\n",
    "                (10, np.NaN, cls.window_test_start_date),\n",
    "                (20, np.NaN, cls.window_test_start_date),\n",
    "                (30, 131, pd.Timestamp('2015-01-09')),\n",
    "                (40, 140., pd.Timestamp('2015-01-09')),\n",
    "                (50, 150., pd.Timestamp('2015-01-09'))\n",
    "            ], pd.Timestamp('2015-01-14')),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute([\n",
    "                    (0, np.NaN, cls.window_test_start_date),\n",
    "                    (10, np.NaN, cls.window_test_start_date),\n",
    "                    (20, np.NaN, cls.window_test_start_date),\n",
    "                    (30, 131*11, pd.Timestamp('2015-01-09')),\n",
    "                    (40, 140., pd.Timestamp('2015-01-09')),\n",
    "                    (50, 150., pd.Timestamp('2015-01-09')),\n",
    "                ], end_date)\n",
    "                for end_date in pd.date_range('2015-01-15', '2015-01-16')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                     (10, np.NaN, cls.window_test_start_date),\n",
    "                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),\n",
    "                     (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                     (40, 140.*13, pd.Timestamp('2015-01-09')),\n",
    "                     (50, 150., pd.Timestamp('2015-01-09'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-20', '2015-01-21')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101, pd.Timestamp('2015-01-20')),\n",
    "                     (10, 111*.3, pd.Timestamp('2015-01-22')),\n",
    "                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),\n",
    "                     (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),\n",
    "                     (50, 150., pd.Timestamp('2015-01-09'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-22', '2015-01-29')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101*7, pd.Timestamp('2015-01-20')),\n",
    "                     (10, 111*.3, pd.Timestamp('2015-01-22')),\n",
    "                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),\n",
    "                     (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),\n",
    "                     (50, 150., pd.Timestamp('2015-01-09'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-30', '2015-02-04')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 101*7, pd.Timestamp('2015-01-20')),\n",
    "                     (10, 311*.3, pd.Timestamp('2015-02-05')),\n",
    "                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),\n",
    "                     (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),\n",
    "                     (50, 150., pd.Timestamp('2015-01-09'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')\n",
    "                ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 201, pd.Timestamp('2015-02-10')),\n",
    "                 (10, 311*.3, pd.Timestamp('2015-02-05')),\n",
    "                 (20, 221*.8*.9, pd.Timestamp('2015-02-10')),\n",
    "                 (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                 (40, 240.*13*14, pd.Timestamp('2015-02-10')),\n",
    "                 (50, 250., pd.Timestamp('2015-02-10'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        twoq_previous = pd.concat(\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, np.NaN, cls.window_test_start_date),\n",
    "                 (30, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-09', '2015-01-19')] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, np.NaN, cls.window_test_start_date),\n",
    "                 (30, 131*11*12, pd.Timestamp('2015-01-20'))],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-20', '2015-02-09')] +\n",
    "            # We never get estimates for S1 for 2Q ago because once Q3\n",
    "            # becomes our previous quarter, 2Q ago would be Q2, and we have\n",
    "            # no data on it.\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 101*7, pd.Timestamp('2015-02-10')),\n",
    "                 (10, np.NaN, pd.Timestamp('2015-02-05')),\n",
    "                 (20, 121*.7*.8*.9, pd.Timestamp('2015-02-10')),\n",
    "                 (30, 131*11*12, pd.Timestamp('2015-01-20')),\n",
    "                 (40, 140. * 13 * 14, pd.Timestamp('2015-02-10')),\n",
    "                 (50, 150., pd.Timestamp('2015-02-10'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            )]\n",
    "        )\n",
    "        return {\n",
    "            1: oneq_previous,\n",
    "            2: twoq_previous\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazePreviousWithSplitAdjustedWindows(PreviousWithSplitAdjustedWindows):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousSplitAdjustedEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof_date,\n",
    "        )\n",
    "\n",
    "\n",
    "class NextWithSplitAdjustedWindows(WithSplitAdjustedWindows, ZiplineTestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextSplitAdjustedEarningsEstimatesLoader(\n",
    "            events,\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof_date,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines(cls):\n",
    "        oneq_next = pd.concat([\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100*1/4, cls.window_test_start_date),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (20, 120*5/3, cls.window_test_start_date),\n",
    "                 (20, 121*5/3, pd.Timestamp('2015-01-07')),\n",
    "                 (30, 130*1/10, cls.window_test_start_date),\n",
    "                 (30, 131*1/10, pd.Timestamp('2015-01-09')),\n",
    "                 (40, 140, pd.Timestamp('2015-01-09')),\n",
    "                 (50, 150.*1/15*1/16, pd.Timestamp('2015-01-09'))],\n",
    "                pd.Timestamp('2015-01-09')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100*1/4, cls.window_test_start_date),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 120*5/3, cls.window_test_start_date),\n",
    "                 (20, 121*5/3, pd.Timestamp('2015-01-07')),\n",
    "                 (30, 230*1/10, cls.window_test_start_date),\n",
    "                 (40, np.NaN, pd.Timestamp('2015-01-10')),\n",
    "                 (50, 250.*1/15*1/16, pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-12')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100, cls.window_test_start_date),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 120, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-07')),\n",
    "                 (30, 230, cls.window_test_start_date),\n",
    "                 (40, np.NaN, pd.Timestamp('2015-01-10')),\n",
    "                 (50, 250.*1/16, pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-13')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100, cls.window_test_start_date),\n",
    "                 (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 120, cls.window_test_start_date),\n",
    "                 (20, 121, pd.Timestamp('2015-01-07')),\n",
    "                 (30, 230, cls.window_test_start_date),\n",
    "                 (40, np.NaN, pd.Timestamp('2015-01-10')),\n",
    "                 (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-14')\n",
    "            ),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 100*5, cls.window_test_start_date),\n",
    "                     (10, 110, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 111, pd.Timestamp('2015-01-12')),\n",
    "                     (20, 120*.7, cls.window_test_start_date),\n",
    "                     (20, 121*.7, pd.Timestamp('2015-01-07')),\n",
    "                     (30, 230*11, cls.window_test_start_date),\n",
    "                     (40, 240, pd.Timestamp('2015-01-15')),\n",
    "                     (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-15', '2015-01-16')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 100*5*6, cls.window_test_start_date),\n",
    "                 (0, 101, pd.Timestamp('2015-01-20')),\n",
    "                 (10, 110*.3, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111*.3, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 120*.7*.8, cls.window_test_start_date),\n",
    "                 (20, 121*.7*.8, pd.Timestamp('2015-01-07')),\n",
    "                 (30, 230*11*12, cls.window_test_start_date),\n",
    "                 (30, 231, pd.Timestamp('2015-01-20')),\n",
    "                 (40, 240*13, pd.Timestamp('2015-01-15')),\n",
    "                 (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-20')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),\n",
    "                 (10, 110 * .3, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111 * .3, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 220 * .7 * .8, cls.window_test_start_date),\n",
    "                 (20, 221 * .8, pd.Timestamp('2015-01-17')),\n",
    "                 (40, 240 * 13, pd.Timestamp('2015-01-15')),\n",
    "                 (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-21')\n",
    "            ),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),\n",
    "                 (10, 110 * .3, pd.Timestamp('2015-01-09')),\n",
    "                 (10, 111 * .3, pd.Timestamp('2015-01-12')),\n",
    "                 (20, 220 * .7 * .8, cls.window_test_start_date),\n",
    "                 (20, 221 * .8, pd.Timestamp('2015-01-17')),\n",
    "                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),\n",
    "                 (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-01-22')\n",
    "            ),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200*5*6, pd.Timestamp('2015-01-12')),\n",
    "                     (10, 310*.3, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 311*.3, pd.Timestamp('2015-01-15')),\n",
    "                     (20, 220*.7*.8, cls.window_test_start_date),\n",
    "                     (20, 221*.8, pd.Timestamp('2015-01-17')),\n",
    "                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),\n",
    "                     (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-23', '2015-01-29')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),\n",
    "                     (10, 310*.3, pd.Timestamp('2015-01-09')),\n",
    "                     (10, 311*.3, pd.Timestamp('2015-01-15')),\n",
    "                     (20, 220*.7*.8*.9, cls.window_test_start_date),\n",
    "                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),\n",
    "                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),\n",
    "                     (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-01-30', '2015-02-05')\n",
    "            ]),\n",
    "            pd.concat([\n",
    "                cls.create_expected_df_for_factor_compute(\n",
    "                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),\n",
    "                     (10, np.NaN, cls.window_test_start_date),\n",
    "                     (20, 220*.7*.8*.9, cls.window_test_start_date),\n",
    "                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),\n",
    "                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),\n",
    "                     (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                    end_date\n",
    "                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')\n",
    "            ]),\n",
    "            cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),\n",
    "                 (0, 201, pd.Timestamp('2015-02-10')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220*.7*.8*.9, cls.window_test_start_date),\n",
    "                 (20, 221*.8*.9, pd.Timestamp('2015-01-17')),\n",
    "                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),\n",
    "                 (50, 250., pd.Timestamp('2015-01-12'))],\n",
    "                pd.Timestamp('2015-02-10')\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        twoq_next = pd.concat(\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220*5/3, cls.window_test_start_date),\n",
    "                 (30, 230*1/10, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date),\n",
    "                 (50, np.NaN, cls.window_test_start_date)],\n",
    "                pd.Timestamp('2015-01-09')\n",
    "            )] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200*1/4, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220*5/3, cls.window_test_start_date),\n",
    "                 (30, np.NaN, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date)],\n",
    "                pd.Timestamp('2015-01-12')\n",
    "            )] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220, cls.window_test_start_date),\n",
    "                 (30, np.NaN, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-13', '2015-01-14')] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200*5, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220*.7, cls.window_test_start_date),\n",
    "                 (30, np.NaN, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-15', '2015-01-16')] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, 200*5*6, pd.Timestamp('2015-01-12')),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, 220*.7*.8, cls.window_test_start_date),\n",
    "                 (20, 221*.8, pd.Timestamp('2015-01-17')),\n",
    "                 (30, np.NaN, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date)],\n",
    "                pd.Timestamp('2015-01-20')\n",
    "            )] +\n",
    "            [cls.create_expected_df_for_factor_compute(\n",
    "                [(0, np.NaN, cls.window_test_start_date),\n",
    "                 (10, np.NaN, cls.window_test_start_date),\n",
    "                 (20, np.NaN, cls.window_test_start_date),\n",
    "                 (30, np.NaN, cls.window_test_start_date),\n",
    "                 (40, np.NaN, cls.window_test_start_date)],\n",
    "                end_date\n",
    "            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            1: oneq_next,\n",
    "            2: twoq_next\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazeNextWithSplitAdjustedWindows(NextWithSplitAdjustedWindows):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextSplitAdjustedEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof_date,\n",
    "        )\n",
    "\n",
    "\n",
    "class WithSplitAdjustedMultipleEstimateColumns(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin for having multiple estimate columns that are\n",
    "    split-adjusted to make sure that adjustments are applied correctly.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    test_start_date : pd.Timestamp\n",
    "        The start date of the test.\n",
    "    test_end_date : pd.Timestamp\n",
    "        The start date of the test.\n",
    "    split_adjusted_asof : pd.Timestamp\n",
    "        The split-adjusted-asof-date of the data used in the test, to be used\n",
    "        to create all loaders of test classes that subclass this mixin.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_expected_timelines_1q_out -> dict[pd.Timestamp -> dict[str ->\n",
    "        np.array]]\n",
    "        The expected array of results for each date of the date range for\n",
    "        each column. Only for 1 quarter out.\n",
    "\n",
    "    make_expected_timelines_2q_out -> dict[pd.Timestamp -> dict[str ->\n",
    "        np.array]]\n",
    "        The expected array of results for each date of the date range. For 2\n",
    "        quarters out, so only for the column that is requested to be loaded\n",
    "        with 2 quarters out.\n",
    "\n",
    "    Tests\n",
    "    -----\n",
    "    test_adjustments_with_multiple_adjusted_columns\n",
    "        Tests that if you have multiple columns, we still split-adjust\n",
    "        correctly.\n",
    "\n",
    "    test_multiple_datasets_different_num_announcements\n",
    "        Tests that if you have multiple datasets that ask for a different\n",
    "        number of quarters out, and each asks for a different estimates column,\n",
    "        we still split-adjust correctly.\n",
    "    \"\"\"\n",
    "    END_DATE = pd.Timestamp('2015-02-10')\n",
    "    test_start_date = pd.Timestamp('2015-01-06', tz='utc')\n",
    "    test_end_date = pd.Timestamp('2015-01-12', tz='utc')\n",
    "    split_adjusted_asof = pd.Timestamp('2015-01-08')\n",
    "\n",
    "    @classmethod\n",
    "    def make_columns(cls):\n",
    "        return {\n",
    "            MultipleColumnsEstimates.event_date: 'event_date',\n",
    "            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',\n",
    "            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',\n",
    "            MultipleColumnsEstimates.estimate1: 'estimate1',\n",
    "            MultipleColumnsEstimates.estimate2: 'estimate2'\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        sid_0_events = pd.DataFrame({\n",
    "            # We only want a stale KD here so that adjustments\n",
    "            # will be applied.\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),\n",
    "                            pd.Timestamp('2015-01-05')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-09'),\n",
    "                 pd.Timestamp('2015-01-12')],\n",
    "            'estimate1': [1100., 1200.],\n",
    "            'estimate2': [2100., 2200.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 0,\n",
    "        })\n",
    "\n",
    "        # This is just an extra sid to make sure that we apply adjustments\n",
    "        # correctly for multiple columns when we have multiple sids.\n",
    "        sid_1_events = pd.DataFrame({\n",
    "            # We only want a stale KD here so that adjustments\n",
    "            # will be applied.\n",
    "            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),\n",
    "                            pd.Timestamp('2015-01-05')],\n",
    "            EVENT_DATE_FIELD_NAME:\n",
    "                [pd.Timestamp('2015-01-08'),\n",
    "                 pd.Timestamp('2015-01-11')],\n",
    "            'estimate1': [1110., 1210.],\n",
    "            'estimate2': [2110., 2210.],\n",
    "            FISCAL_QUARTER_FIELD_NAME: [1, 2],\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 1,\n",
    "        })\n",
    "        return pd.concat([sid_0_events, sid_1_events])\n",
    "\n",
    "    @classmethod\n",
    "    def make_splits_data(cls):\n",
    "        sid_0_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 0,\n",
    "            'ratio': (.3, 3.),\n",
    "            'effective_date': (pd.Timestamp('2015-01-07'),\n",
    "                               pd.Timestamp('2015-01-09')),\n",
    "        })\n",
    "\n",
    "        sid_1_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 1,\n",
    "            'ratio': (.4, 4.),\n",
    "            'effective_date': (pd.Timestamp('2015-01-07'),\n",
    "                               pd.Timestamp('2015-01-09')),\n",
    "        })\n",
    "\n",
    "        return pd.concat([sid_0_splits, sid_1_splits])\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_1q_out(cls):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_2q_out(cls):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(\n",
    "            WithSplitAdjustedMultipleEstimateColumns, cls\n",
    "        ).init_class_fixtures()\n",
    "        cls.timelines_1q_out = cls.make_expected_timelines_1q_out()\n",
    "        cls.timelines_2q_out = cls.make_expected_timelines_2q_out()\n",
    "\n",
    "    def test_adjustments_with_multiple_adjusted_columns(self):\n",
    "        dataset = MultipleColumnsQuartersEstimates(1)\n",
    "        timelines = self.timelines_1q_out\n",
    "        window_len = 3\n",
    "\n",
    "        class SomeFactor(CustomFactor):\n",
    "            inputs = [dataset.estimate1, dataset.estimate2]\n",
    "            window_length = window_len\n",
    "\n",
    "            def compute(self, today, assets, out, estimate1, estimate2):\n",
    "                assert_almost_equal(estimate1, timelines[today]['estimate1'])\n",
    "                assert_almost_equal(estimate2, timelines[today]['estimate2'])\n",
    "\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        engine.run_pipeline(\n",
    "            Pipeline({'est': SomeFactor()}),\n",
    "            start_date=self.test_start_date,\n",
    "            # last event date we have\n",
    "            end_date=self.test_end_date,\n",
    "        )\n",
    "\n",
    "    def test_multiple_datasets_different_num_announcements(self):\n",
    "        dataset1 = MultipleColumnsQuartersEstimates(1)\n",
    "        dataset2 = MultipleColumnsQuartersEstimates(2)\n",
    "        timelines_1q_out = self.timelines_1q_out\n",
    "        timelines_2q_out = self.timelines_2q_out\n",
    "        window_len = 3\n",
    "\n",
    "        class SomeFactor1(CustomFactor):\n",
    "            inputs = [dataset1.estimate1]\n",
    "            window_length = window_len\n",
    "\n",
    "            def compute(self, today, assets, out, estimate1):\n",
    "                assert_almost_equal(\n",
    "                    estimate1, timelines_1q_out[today]['estimate1']\n",
    "                )\n",
    "\n",
    "        class SomeFactor2(CustomFactor):\n",
    "            inputs = [dataset2.estimate2]\n",
    "            window_length = window_len\n",
    "\n",
    "            def compute(self, today, assets, out, estimate2):\n",
    "                assert_almost_equal(\n",
    "                    estimate2, timelines_2q_out[today]['estimate2']\n",
    "                )\n",
    "\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: self.loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        engine.run_pipeline(\n",
    "            Pipeline({'est1': SomeFactor1(), 'est2': SomeFactor2()}),\n",
    "            start_date=self.test_start_date,\n",
    "            # last event date we have\n",
    "            end_date=self.test_end_date,\n",
    "        )\n",
    "\n",
    "\n",
    "class PreviousWithSplitAdjustedMultipleEstimateColumns(\n",
    "    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase\n",
    "):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return PreviousSplitAdjustedEarningsEstimatesLoader(\n",
    "            events,\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate1', 'estimate2'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_1q_out(cls):\n",
    "        return {\n",
    "            pd.Timestamp('2015-01-06', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] * 3),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-07', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] * 3),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-08', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] * 2 +\n",
    "                                      [[np.NaN, 1110.]]),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +\n",
    "                                      [[np.NaN, 2110.]])\n",
    "            },\n",
    "            pd.Timestamp('2015-01-09', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] +\n",
    "                                      [[np.NaN, 1110. * 4]] +\n",
    "                                      [[1100 * 3., 1110. * 4]]),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] +\n",
    "                                      [[np.NaN, 2110. * 4]] +\n",
    "                                      [[2100 * 3., 2110. * 4]])\n",
    "            },\n",
    "            pd.Timestamp('2015-01-12', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] * 2 +\n",
    "                                      [[1200 * 3., 1210. * 4]]),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +\n",
    "                                      [[2200 * 3., 2210. * 4]])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_2q_out(cls):\n",
    "        return {\n",
    "            pd.Timestamp('2015-01-06', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-07', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-08', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-09', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-12', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +\n",
    "                                      [[2100 * 3., 2110. * 4]])\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazePreviousWithMultipleEstimateColumns(\n",
    "    PreviousWithSplitAdjustedMultipleEstimateColumns\n",
    "):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazePreviousSplitAdjustedEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate1', 'estimate2'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof,\n",
    "        )\n",
    "\n",
    "\n",
    "class NextWithSplitAdjustedMultipleEstimateColumns(\n",
    "    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase\n",
    "):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return NextSplitAdjustedEarningsEstimatesLoader(\n",
    "            events,\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate1', 'estimate2'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_1q_out(cls):\n",
    "        return {\n",
    "            pd.Timestamp('2015-01-06', tz='utc'): {\n",
    "                'estimate1': np.array([[np.NaN, np.NaN]] +\n",
    "                                      [[1100. * 1/.3, 1110. * 1/.4]] * 2),\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] +\n",
    "                                      [[2100. * 1/.3, 2110. * 1/.4]] * 2),\n",
    "            },\n",
    "            pd.Timestamp('2015-01-07', tz='utc'): {\n",
    "                'estimate1': np.array([[1100., 1110.]] * 3),\n",
    "                'estimate2': np.array([[2100., 2110.]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-08', tz='utc'): {\n",
    "                'estimate1': np.array([[1100., 1110.]] * 3),\n",
    "                'estimate2': np.array([[2100., 2110.]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-09', tz='utc'): {\n",
    "                'estimate1': np.array([[1100 * 3., 1210. * 4]] * 3),\n",
    "                'estimate2': np.array([[2100 * 3., 2210. * 4]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-12', tz='utc'): {\n",
    "                'estimate1': np.array([[1200 * 3., np.NaN]] * 3),\n",
    "                'estimate2': np.array([[2200 * 3., np.NaN]] * 3)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_timelines_2q_out(cls):\n",
    "        return {\n",
    "            pd.Timestamp('2015-01-06', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] +\n",
    "                                      [[2200 * 1/.3, 2210. * 1/.4]] * 2)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-07', tz='utc'): {\n",
    "                'estimate2': np.array([[2200., 2210.]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-08', tz='utc'): {\n",
    "                'estimate2': np.array([[2200, 2210.]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-09', tz='utc'): {\n",
    "                'estimate2': np.array([[2200 * 3., np.NaN]] * 3)\n",
    "            },\n",
    "            pd.Timestamp('2015-01-12', tz='utc'): {\n",
    "                'estimate2': np.array([[np.NaN, np.NaN]] * 3)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class BlazeNextWithMultipleEstimateColumns(\n",
    "    NextWithSplitAdjustedMultipleEstimateColumns\n",
    "):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return BlazeNextSplitAdjustedEstimatesLoader(\n",
    "            bz.data(events),\n",
    "            columns,\n",
    "            split_adjustments_loader=cls.adjustment_reader,\n",
    "            split_adjusted_column_names=['estimate1', 'estimate2'],\n",
    "            split_adjusted_asof=cls.split_adjusted_asof,\n",
    "        )\n",
    "\n",
    "\n",
    "class WithAdjustmentBoundaries(WithEstimates):\n",
    "    \"\"\"\n",
    "    ZiplineTestCase mixin providing class-level attributes, methods,\n",
    "    and a test to make sure that when the split-adjusted-asof-date is not\n",
    "    strictly within the date index, we can still apply adjustments correctly.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    split_adjusted_before_start : pd.Timestamp\n",
    "        A split-adjusted-asof-date before the start date of the test.\n",
    "    split_adjusted_after_end : pd.Timestamp\n",
    "        A split-adjusted-asof-date before the end date of the test.\n",
    "    split_adjusted_asof_dates : list of tuples of pd.Timestamp\n",
    "        All the split-adjusted-asof-dates over which we want to parameterize\n",
    "        the test.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    make_expected_out -> dict[pd.Timestamp -> pd.DataFrame]\n",
    "        A dictionary of the expected output of the pipeline at each of the\n",
    "        dates of interest.\n",
    "    \"\"\"\n",
    "    START_DATE = pd.Timestamp('2015-01-04')\n",
    "    # We want to run the pipeline starting from `START_DATE`, but the\n",
    "    # pipeline results will start from the next day, which is\n",
    "    # `test_start_date`.\n",
    "    test_start_date = pd.Timestamp('2015-01-05')\n",
    "    END_DATE = test_end_date = pd.Timestamp('2015-01-12')\n",
    "    split_adjusted_before_start = (\n",
    "        test_start_date - timedelta(days=1)\n",
    "    )\n",
    "    split_adjusted_after_end = (\n",
    "        test_end_date + timedelta(days=1)\n",
    "    )\n",
    "    # Must parametrize over this because there can only be 1 such date for\n",
    "    # each set of data.\n",
    "    split_adjusted_asof_dates = [(test_start_date,),\n",
    "                                 (test_end_date,),\n",
    "                                 (split_adjusted_before_start,),\n",
    "                                 (split_adjusted_after_end,)]\n",
    "\n",
    "    @classmethod\n",
    "    def init_class_fixtures(cls):\n",
    "        super(WithAdjustmentBoundaries, cls).init_class_fixtures()\n",
    "        cls.s0 = cls.asset_finder.retrieve_asset(0)\n",
    "        cls.s1 = cls.asset_finder.retrieve_asset(1)\n",
    "        cls.s2 = cls.asset_finder.retrieve_asset(2)\n",
    "        cls.s3 = cls.asset_finder.retrieve_asset(3)\n",
    "        cls.s4 = cls.asset_finder.retrieve_asset(4)\n",
    "        cls.expected = cls.make_expected_out()\n",
    "\n",
    "    @classmethod\n",
    "    def make_events(cls):\n",
    "        # We can create a sid for each configuration of dates for KDs, events,\n",
    "        # and splits. For this test we don't care about overwrites so we only\n",
    "        # test 1 quarter.\n",
    "        sid_0_timeline = pd.DataFrame({\n",
    "            # KD on first date of index\n",
    "            TS_FIELD_NAME: cls.test_start_date,\n",
    "            EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-09'),\n",
    "            'estimate': 10.,\n",
    "            FISCAL_QUARTER_FIELD_NAME: 1,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 0,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_1_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: cls.test_start_date,\n",
    "            # event date on first date of index\n",
    "            EVENT_DATE_FIELD_NAME: cls.test_start_date,\n",
    "            'estimate': 11.,\n",
    "            FISCAL_QUARTER_FIELD_NAME: 1,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 1,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_2_timeline = pd.DataFrame({\n",
    "            # KD on first date of index\n",
    "            TS_FIELD_NAME: cls.test_end_date,\n",
    "            EVENT_DATE_FIELD_NAME: cls.test_end_date + timedelta(days=1),\n",
    "            'estimate': 12.,\n",
    "            FISCAL_QUARTER_FIELD_NAME: 1,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 2,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_3_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),\n",
    "            EVENT_DATE_FIELD_NAME: cls.test_end_date,\n",
    "            'estimate': 13.,\n",
    "            FISCAL_QUARTER_FIELD_NAME: 1,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 3,\n",
    "        }, index=[0])\n",
    "\n",
    "        # KD and event date don't fall on date index boundaries\n",
    "        sid_4_timeline = pd.DataFrame({\n",
    "            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),\n",
    "            EVENT_DATE_FIELD_NAME: cls.test_end_date - timedelta(days=1),\n",
    "            'estimate': 14.,\n",
    "            FISCAL_QUARTER_FIELD_NAME: 1,\n",
    "            FISCAL_YEAR_FIELD_NAME: 2015,\n",
    "            SID_FIELD_NAME: 4,\n",
    "        }, index=[0])\n",
    "\n",
    "        return pd.concat([sid_0_timeline,\n",
    "                          sid_1_timeline,\n",
    "                          sid_2_timeline,\n",
    "                          sid_3_timeline,\n",
    "                          sid_4_timeline])\n",
    "\n",
    "    @classmethod\n",
    "    def make_splits_data(cls):\n",
    "        # Here we want splits that collide\n",
    "        sid_0_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 0,\n",
    "            'ratio': .10,\n",
    "            'effective_date': cls.test_start_date,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_1_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 1,\n",
    "            'ratio': .11,\n",
    "            'effective_date': cls.test_start_date,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_2_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 2,\n",
    "            'ratio': .12,\n",
    "            'effective_date': cls.test_end_date,\n",
    "        }, index=[0])\n",
    "\n",
    "        sid_3_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 3,\n",
    "            'ratio': .13,\n",
    "            'effective_date': cls.test_end_date,\n",
    "        }, index=[0])\n",
    "\n",
    "        # We want 2 splits here - at the starting boundary and at the end\n",
    "        # boundary - while there is no collision with KD/event date for the\n",
    "        # sid.\n",
    "        sid_4_splits = pd.DataFrame({\n",
    "            SID_FIELD_NAME: 4,\n",
    "            'ratio': (.14, .15),\n",
    "            'effective_date': (cls.test_start_date, cls.test_end_date),\n",
    "        })\n",
    "\n",
    "        return pd.concat([sid_0_splits,\n",
    "                          sid_1_splits,\n",
    "                          sid_2_splits,\n",
    "                          sid_3_splits,\n",
    "                          sid_4_splits])\n",
    "\n",
    "    @parameterized.expand(split_adjusted_asof_dates)\n",
    "    def test_boundaries(self, split_date):\n",
    "        dataset = QuartersEstimates(1)\n",
    "        loader = self.loader(split_adjusted_asof=split_date)\n",
    "        engine = SimplePipelineEngine(\n",
    "            lambda x: loader,\n",
    "            self.trading_days,\n",
    "            self.asset_finder,\n",
    "        )\n",
    "        result = engine.run_pipeline(\n",
    "            Pipeline({'estimate': dataset.estimate.latest}),\n",
    "            start_date=self.trading_days[0],\n",
    "            # last event date we have\n",
    "            end_date=self.trading_days[-1],\n",
    "        )\n",
    "        expected = self.expected[split_date]\n",
    "        assert_frame_equal(result, expected, check_names=False)\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        return {}\n",
    "\n",
    "\n",
    "class PreviousWithAdjustmentBoundaries(WithAdjustmentBoundaries,\n",
    "                                       ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return partial(PreviousSplitAdjustedEarningsEstimatesLoader,\n",
    "                       events,\n",
    "                       columns,\n",
    "                       split_adjustments_loader=cls.adjustment_reader,\n",
    "                       split_adjusted_column_names=['estimate'])\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        split_adjusted_at_start_boundary = pd.concat([\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': np.NaN,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date,\n",
    "                pd.Timestamp('2015-01-08'),\n",
    "                tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': 10.,\n",
    "            }, index=pd.date_range(\n",
    "                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s1,\n",
    "                'estimate': 11.,\n",
    "            }, index=pd.date_range(cls.test_start_date, cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s2,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(cls.test_start_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': 13. * .13\n",
    "            }, index=pd.date_range(cls.test_end_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, cls.test_end_date - timedelta(2), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': 14. * .15\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'\n",
    "            )),\n",
    "        ]).set_index(SID_FIELD_NAME, append=True).unstack(\n",
    "            SID_FIELD_NAME).reindex(cls.trading_days).stack(\n",
    "            SID_FIELD_NAME, dropna=False)\n",
    "\n",
    "        split_adjusted_at_end_boundary = pd.concat([\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': np.NaN,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, pd.Timestamp('2015-01-08'), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': 10.,\n",
    "            }, index=pd.date_range(\n",
    "                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s1,\n",
    "                'estimate': 11.,\n",
    "            }, index=pd.date_range(cls.test_start_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s2,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(cls.test_start_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': 13.\n",
    "            }, index=pd.date_range(cls.test_end_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': np.NaN\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, cls.test_end_date - timedelta(2), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': 14.\n",
    "            }, index=pd.date_range(cls.test_end_date - timedelta(1),\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "        ]).set_index(SID_FIELD_NAME, append=True).unstack(\n",
    "            SID_FIELD_NAME).reindex(cls.trading_days).stack(SID_FIELD_NAME,\n",
    "                                                            dropna=False)\n",
    "\n",
    "        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary\n",
    "        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary\n",
    "\n",
    "        return {cls.test_start_date:\n",
    "                split_adjusted_at_start_boundary,\n",
    "                cls.split_adjusted_before_start:\n",
    "                split_adjusted_before_start_boundary,\n",
    "                cls.test_end_date:\n",
    "                split_adjusted_at_end_boundary,\n",
    "                cls.split_adjusted_after_end:\n",
    "                split_adjusted_after_end_boundary}\n",
    "\n",
    "\n",
    "class BlazePreviousWithAdjustmentBoundaries(PreviousWithAdjustmentBoundaries):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return partial(BlazePreviousSplitAdjustedEstimatesLoader,\n",
    "                       bz.data(events),\n",
    "                       columns,\n",
    "                       split_adjustments_loader=cls.adjustment_reader,\n",
    "                       split_adjusted_column_names=['estimate'])\n",
    "\n",
    "\n",
    "class NextWithAdjustmentBoundaries(WithAdjustmentBoundaries,\n",
    "                                   ZiplineTestCase):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return partial(NextSplitAdjustedEarningsEstimatesLoader,\n",
    "                       events,\n",
    "                       columns,\n",
    "                       split_adjustments_loader=cls.adjustment_reader,\n",
    "                       split_adjusted_column_names=['estimate'])\n",
    "\n",
    "    @classmethod\n",
    "    def make_expected_out(cls):\n",
    "        split_adjusted_at_start_boundary = pd.concat([\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': 10,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s1,\n",
    "                'estimate': 11.,\n",
    "            }, index=pd.date_range(cls.test_start_date,\n",
    "                                   cls.test_start_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s2,\n",
    "                'estimate': 12.,\n",
    "            }, index=pd.date_range(cls.test_end_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': 13. * .13,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': 14.,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_end_date - timedelta(1),\n",
    "                cls.test_end_date - timedelta(1),\n",
    "                tz='utc'\n",
    "            )),\n",
    "        ]).set_index(SID_FIELD_NAME, append=True).unstack(\n",
    "            SID_FIELD_NAME).reindex(cls.trading_days).stack(\n",
    "            SID_FIELD_NAME, dropna=False)\n",
    "\n",
    "        split_adjusted_at_end_boundary = pd.concat([\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s0,\n",
    "                'estimate': 10,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s1,\n",
    "                'estimate': 11.,\n",
    "            }, index=pd.date_range(cls.test_start_date,\n",
    "                                   cls.test_start_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s2,\n",
    "                'estimate': 12.,\n",
    "            }, index=pd.date_range(cls.test_end_date,\n",
    "                                   cls.test_end_date,\n",
    "                                   tz='utc')),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s3,\n",
    "                'estimate': 13.,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'\n",
    "            )),\n",
    "            pd.DataFrame({\n",
    "                SID_FIELD_NAME: cls.s4,\n",
    "                'estimate': 14.,\n",
    "            }, index=pd.date_range(\n",
    "                cls.test_end_date - timedelta(1),\n",
    "                cls.test_end_date - timedelta(1),\n",
    "                tz='utc'\n",
    "            )),\n",
    "        ]).set_index(SID_FIELD_NAME, append=True).unstack(\n",
    "            SID_FIELD_NAME).reindex(cls.trading_days).stack(\n",
    "            SID_FIELD_NAME, dropna=False)\n",
    "\n",
    "        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary\n",
    "        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary\n",
    "\n",
    "        return {cls.test_start_date:\n",
    "                split_adjusted_at_start_boundary,\n",
    "                cls.split_adjusted_before_start:\n",
    "                split_adjusted_before_start_boundary,\n",
    "                cls.test_end_date:\n",
    "                split_adjusted_at_end_boundary,\n",
    "                cls.split_adjusted_after_end:\n",
    "                split_adjusted_after_end_boundary}\n",
    "\n",
    "\n",
    "class BlazeNextWithAdjustmentBoundaries(NextWithAdjustmentBoundaries):\n",
    "    @classmethod\n",
    "    def make_loader(cls, events, columns):\n",
    "        return partial(BlazeNextSplitAdjustedEstimatesLoader,\n",
    "                       bz.data(events),\n",
    "                       columns,\n",
    "                       split_adjustments_loader=cls.adjustment_reader,\n",
    "                       split_adjusted_column_names=['estimate'])\n",
    "\n",
    "\n",
    "class QuarterShiftTestCase(ZiplineTestCase):\n",
    "    \"\"\"\n",
    "    This tests, in isolation, quarter calculation logic for shifting quarters\n",
    "    backwards/forwards from a starting point.\n",
    "    \"\"\"\n",
    "    def test_quarter_normalization(self):\n",
    "        input_yrs = pd.Series(range(2011, 2015), dtype=np.int64)\n",
    "        input_qtrs = pd.Series(range(1, 5), dtype=np.int64)\n",
    "        result_years, result_quarters = split_normalized_quarters(\n",
    "            normalize_quarters(input_yrs, input_qtrs)\n",
    "        )\n",
    "        # Can't use assert_series_equal here with check_names=False\n",
    "        # because that still fails due to name differences.\n",
    "        assert_equal(input_yrs, result_years)\n",
    "        assert_equal(input_qtrs, result_quarters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zipline",
   "language": "python",
   "name": "zipline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
